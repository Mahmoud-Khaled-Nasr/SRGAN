{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahmoud-Khaled-Nasr/SRGAN/blob/master/SRGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3PVr9IyiMkud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#SRGAN, Single Image Super Resolution \n",
        "\n",
        "This notebook is an implementation for the paper \"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\" using Pytorch framework and trained on Google Colaboratory.\n",
        "\n",
        "## SRGAN\n",
        "The SRGAN as any GAN network is made of two networks: \n",
        "\n",
        "*   **discriminator**: It is reponsible for discrminating between real HR (high resolution) images and the SR (super resolution) images generated by the generator \n",
        "*   **generator**: It is responsible for upscaling a LR (low resolution) image and convince the discriminator that it is a real HR image\n",
        "\n",
        "## Notebook structure \n",
        "\n",
        "\n",
        "1.   Loading the images using custom Pytorch Dataset loader to pair each LR image with the right HR image in the data loader\n",
        "2.   Defining the hyperparameters\n",
        "3.   Create the Data loaders\n",
        "4.   Define the model architecture of both descriminator and generator\n",
        "5.   Define the loss functions for the descriminator and generator\n",
        "6.   Training code\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GXMGx5iEdHIq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "###############################\n",
        "# Checking for GPU\n",
        "###############################\n",
        "\n",
        "import torch\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "\n",
        "###############################\n",
        "# Network Parameter\n",
        "###############################\n",
        "BATCH_SIZE = 16\n",
        "EPOCH_NUM = 40\n",
        "###############################\n",
        "# Discriminator\n",
        "###############################\n",
        "DISCRIMINATOR_FINAL_FEATURE_MAP_SIZE = 10\n",
        "###############################\n",
        "# Generator \n",
        "###############################\n",
        "# Number of the residual blocks in the generator\n",
        "RESIDUAL_BLOCKS = 16\n",
        "# Number of upsampling blocks in the generator.\n",
        "# Each block upscale the previous block by a factor of 2\n",
        "UPSAMPLING_BLOCKS = 2\n",
        "###############################\n",
        "# Optimizers\n",
        "###############################\n",
        "lr = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PG33ZxCCmIcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Data Loading\n",
        "##Custom Image Dataset Loader\n",
        "`ImageDataset` is a class that implements Pytorch `Dataset` class to load HR and LR images and pair them correctly.\n",
        "\n",
        "The constructor has the following parameters:\n",
        "* `LR_root_dir`: The base directory for all the LR images\n",
        "* `HR_root_dir`: The base directory for all the HR images\n",
        "* `HR_transform`: Pytorch transform, The HR images transfromation \n",
        "* `LR_transform`: Pytorch transform, The LR images transfromation \n",
        "\n",
        "`__getitem__` returns a pair of (LR image, HR image)"
      ]
    },
    {
      "metadata": {
        "id": "oi4GcFGfvpl5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Custom Image Dataset loader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, LR_root_dir, HR_root_dir, HR_transform, LR_transform):\n",
        "        \n",
        "      self.LR_root_dir = LR_root_dir\n",
        "      self.HR_root_dir = HR_root_dir\n",
        "      \n",
        "      self.LR_transform = LR_transform\n",
        "      self.HR_transform = HR_transform\n",
        "      \n",
        "      import os\n",
        "      self.images_list = os.listdir(LR_root_dir) \n",
        "      self.length = len(self.images_list)\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.length\n",
        "\n",
        "    \n",
        "    def pil_loader(self, path):\n",
        "      # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "      from PIL import Image\n",
        "      with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      import os\n",
        "      # Read the images\n",
        "      img_name = os.path.join(self.LR_root_dir, self.images_list[idx])\n",
        "      LR_image = self.pil_loader(img_name)\n",
        "      img_name = os.path.join(self.HR_root_dir, self.images_list[idx])\n",
        "      HR_image = self.pil_loader(img_name)\n",
        "      \n",
        "      # Apply the transformation to the images\n",
        "      LR_image = self.LR_transform(LR_image)\n",
        "      HR_image = self.HR_transform(HR_image)\n",
        "\n",
        "      # Return pair of images (LR, HR)\n",
        "      return LR_image, HR_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G2vACkfXsN5A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Data loaders\n",
        "Create two data loaders `train_loader` and `validation_loader`"
      ]
    },
    {
      "metadata": {
        "id": "jQ4vLRUQqLEC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Create The DataLoaders\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "\n",
        "train_HR = \"train/HR\"\n",
        "train_LR = \"train/LR\"\n",
        "validation_HR = \"valid/HR\"\n",
        "validation_LR = \"valid/LR\"\n",
        "\n",
        "HR_image_size = (256, 256)\n",
        "LR_image_size = (64, 64)\n",
        "\n",
        "train_LR_transform = transforms.Compose([\n",
        "    transforms.Resize(LR_image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Normailze the HR images between [-1, 1]\n",
        "train_HR_transform = transforms.Compose([\n",
        "    transforms.Resize(HR_image_size),\n",
        "    transforms.ToTensor()\n",
        "    #transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "validation_LR_transform = transforms.Compose([\n",
        "    transforms.Resize(LR_image_size),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Normailze the HR images between [-1, 1]\n",
        "validation_HR_transform = transforms.Compose([\n",
        "    transforms.Resize(HR_image_size),\n",
        "    transforms.ToTensor()\n",
        "    #transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "dataset = ImageDataset(HR_root_dir=train_HR, LR_root_dir=train_LR, HR_transform=train_HR_transform, LR_transform=train_LR_transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "#dataset = ImageDataset(HR_root_dir=validation_HR, LR_root_dir=validation_LR, HR_transform=validation_HR_transform, LR_transform=validation_LR_transform)\n",
        "#validation_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XFskqvSV3nJM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Display Sample of Training Images\n",
        "Below function `imshow` is used to reshape some given images and converts them to NumPy images so that they can be displayed by `plt`. This should display two grids containing a batch of image data (LR, HR)"
      ]
    },
    {
      "metadata": {
        "id": "JeE4zssl329n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# helper imshow function\n",
        "def imshow(img):\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    \n",
        "\n",
        "# get some images from X\n",
        "dataiter = iter(train_loader)\n",
        "# the \"_\" is a placeholder for no labels\n",
        "LR, HR = dataiter.next()\n",
        "\n",
        "# show images\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "# Display the LR images\n",
        "imshow(torchvision.utils.make_grid(LR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwiaUeUb_H2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(16, 12))\n",
        "# Display the HR images\n",
        "imshow(torchvision.utils.make_grid(HR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KTb9CGb4sk06",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model Architecture\n",
        "![Model Architecture](https://drive.google.com/uc?export=view&id=1qjzJAeJ16fFcQcEfsjbEzr4aAWcLCY3p)\n",
        "\n",
        "## Generator\n",
        "The generator is a fully convolutional network so it can accept any image size and the output is an upscaled image with a factor that depends on the number of upscaling blocks. Each upscale block provides upscale factor of 2.\n",
        "\n",
        "The generator components are:\n",
        "* `ResidualBlock`\n",
        "* `UpscaleBlock`\n",
        "* tanh activation function\n",
        "\n",
        "### Residual Blocks\n",
        "Based on the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) residual blocks have proven very effective aganist the degradation  problems due to deep network architectures.\n",
        "\n",
        "Each block consists of:\n",
        "\n",
        "\n",
        "1.   Convolution layer with $kernel = 3$, $stride = 1$ and $padding = 1$\n",
        "2.   Batch normalization\n",
        "3.   Parametric Relu\n",
        "4.   Convolution layer with $kernel = 3$, $stride = 1$ and $padding = 1$\n",
        "5.   Batch normalization \n",
        "\n",
        "Let the result of the previous layers is $y$ for input $x$, so the output of the block is $R = x + y$\n",
        "\n",
        "### Upscale Blocks\n",
        "Instead of the normal de-convolution layer we are using pixel shuffle.\n",
        "\n",
        "Each block consists of:\n",
        "\n",
        "\n",
        "1.   Convolution layer with $out\\ channels = in\\ channels * 2^2$ where 2 is the upscale factor\n",
        "2.   Pixel shuffle layer\n",
        "3.   Parametric Relu as activation function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GqZCDAHKtjMU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Generator Model\n",
        "from torch import nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self, channels, kernel_size=3, stride=1, padding=1):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(channels, channels, kernel_size, stride=stride, padding=padding)\n",
        "    self.bn1 = nn.BatchNorm2d(channels)\n",
        "    self.prelu = nn.PReLU()\n",
        "    self.conv2 = nn.Conv2d(channels, channels, kernel_size, stride=stride, padding=padding)\n",
        "    self.bn2 = nn.BatchNorm2d(channels)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    result = self.bn2(self.conv2(self.prelu(self.bn1(self.conv1(x)))))\n",
        "    assert result.shape == x.shape\n",
        "    return result + x\n",
        "\n",
        "\n",
        "class UpscaleBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_channels, upscale_factor):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, in_channels * (upscale_factor ** 2), kernel_size=3, stride=1, padding=1)\n",
        "    self.ps = nn.PixelShuffle(upscale_factor)\n",
        "    self.prelu = nn.PReLU()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.prelu(self.ps(self.conv(x)))\n",
        "    \n",
        "  \n",
        "class Generator(nn.Module):\n",
        "  \n",
        "  def __init__(self, residual_block_number, upscaling_block_number):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.residual_block_channels = 64\n",
        "    self.input_channels = 3\n",
        "    self.scale_factor = 2\n",
        "    \n",
        "    self.input_block = nn.Sequential(\n",
        "        nn.Conv2d(self.input_channels, self.residual_block_channels, kernel_size=9, stride=1, padding=4),\n",
        "        nn.PReLU()\n",
        "    ) \n",
        "    \n",
        "    self.residual_blocks = nn.ModuleList()\n",
        "    for _ in range(residual_block_number):\n",
        "      self.residual_blocks.append(ResidualBlock(self.residual_block_channels))\n",
        "    \n",
        "    self.residual_output = nn.Sequential(\n",
        "        nn.Conv2d(self.residual_block_channels, self.residual_block_channels, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(self.residual_block_channels)\n",
        "    )\n",
        "    \n",
        "    self.upscaling_blocks = nn.ModuleList()\n",
        "    for _ in range(upscaling_block_number):\n",
        "      self.upscaling_blocks.append(UpscaleBlock(self.residual_block_channels, self.scale_factor))\n",
        "    \n",
        "    self.output_layer = nn.Conv2d(self.residual_block_channels, 3, kernel_size=9, stride=1, padding=4)\n",
        "    \n",
        "    self.tanh = nn.Tanh()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    input_block_output = self.input_block(x)\n",
        "    result = input_block_output\n",
        "    for block in self.residual_blocks:\n",
        "      result = block(result)\n",
        "    \n",
        "    result = self.residual_output(result) + input_block_output\n",
        "    \n",
        "    for block in self.upscaling_blocks:\n",
        "      result = block(result)\n",
        "      \n",
        "    return self.tanh(self.output_layer(result))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfWgi7smN53U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Discriminator\n",
        "The discriminator consists of\n",
        "\n",
        "1.   Series of convolution layers with leaky relu as the activation function\n",
        "2.   Fully connected network followed by a sigmoid activation function to calculate the probability of the image being a real of fake\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F8qvAPbmx5QA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Discriminator Model\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "      super().__init__()\n",
        "      alpha = 0.2\n",
        "      \n",
        "      self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride)\n",
        "      self.bn = nn.BatchNorm2d(out_channels)\n",
        "      self.lrelu = nn.LeakyReLU(alpha)\n",
        "    \n",
        "    def forward(self, x):\n",
        "      result = self.conv(x)\n",
        "      result = self.bn(result)\n",
        "      return self.lrelu(result)\n",
        "\n",
        "    \n",
        "class Flatten(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return x.view(x.shape[0], -1)\n",
        "  \n",
        "class Discriminator(nn.Module):  \n",
        "  \n",
        "  def __init__(self, final_feature_map_size):\n",
        "    super().__init__()\n",
        "    alpha = 0.2\n",
        "    assert final_feature_map_size > 0\n",
        "    \n",
        "    self.input_block = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=3, stride=1),\n",
        "        nn.LeakyReLU(alpha)\n",
        "    )\n",
        "    \n",
        "    self.blocks = nn.Sequential(\n",
        "        ConvBlock(64, 64, 2),\n",
        "        ConvBlock(64, 128, 1),\n",
        "        ConvBlock(128, 128, 2),\n",
        "        ConvBlock(128, 256, 1),\n",
        "        ConvBlock(256, 256, 2),\n",
        "        ConvBlock(256, 512, 1),\n",
        "        ConvBlock(512, 512, 2),\n",
        "    )\n",
        "    \n",
        "    img_size = final_feature_map_size\n",
        "    dense_block_input_size = 512 * img_size * img_size\n",
        "    \n",
        "    self.output_block = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(img_size),\n",
        "        Flatten(),\n",
        "        nn.Linear(dense_block_input_size, 1024),\n",
        "        nn.LeakyReLU(alpha),\n",
        "        nn.Linear(1024, 1)\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    assert x.shape[2] >= 64 and x.shape[3] >= 64\n",
        "    return self.output_block(self.blocks(self.input_block(x)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33c7ujBaSimG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Loss\n",
        "The goal of this paper is super resolution with an eye on the perceptual properties of the final image. It must be photo-realistic as much as possible.\n",
        "\n",
        "##Generator Loss\n",
        "The paper is using the following loss\n",
        "$$Loss = Perception\\ Loss + 0.001 * Advesrial\\ Loss$$\n",
        "but through the training and experimentation the training converged much faster when adding the mean square error of the images' pixel values to the equation\n",
        "$$Loss = Image\\ Loss + Perception\\ Loss + 0.001 * Advesrial\\ Loss$$\n",
        "\n",
        "###Advesrial Loss\n",
        "This measures the ability of the generator to fool the discriminator. Calculated by BCE (Binary Cross Entrophy).\n",
        "\n",
        "###Image Loss\n",
        "This measures how much the two images are away from each other in terms of intensity. Calculated as MSE (mean square error).\n",
        "\n",
        "###Perception Loss\n",
        "This measures the difference in high frequency features and perceptual properties of the SR image to make it photo-realistic as much as possible. Using **VGG19** network to extract feature maps for each of HR and SR images we can compare the feature maps using MSE and enhance the generator based on it.\n"
      ]
    },
    {
      "metadata": {
        "id": "qGRAYA_iLIlV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Generator Loss { form-width: \"150px\" }\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = models.vgg19(pretrained=True)\n",
        "    model.eval()\n",
        "    \n",
        "    fifth_conv_layer_index = 26\n",
        "    features = model.features\n",
        "    self.feature_map_extractor = nn.Sequential(*list(model.features)[:fifth_conv_layer_index+1])\n",
        "    self.feature_map_extractor.eval()\n",
        "    for param in self.feature_map_extractor.parameters():\n",
        "      param.requires_grad = False\n",
        "      \n",
        "    self.mse = nn.MSELoss()\n",
        "       \n",
        "    \n",
        "  def forward(self, real_image, generated_image):\n",
        "    assert real_image.shape == generated_image.shape\n",
        "    \n",
        "    loss = self.mse(self.feature_map_extractor(generated_image), self.feature_map_extractor(real_image))\n",
        "    \n",
        "    return loss\n",
        "  \n",
        "  \n",
        "class GeneratorLoss(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.perceptual_loss = PerceptualLoss()\n",
        "    self.discrimenator_loss = nn.BCEWithLogitsLoss()\n",
        "    self.image_loss = nn.MSELoss()\n",
        "    \n",
        "  def forward(self, real_imges, generated_images, output_labels, target_labels):\n",
        "    self.perc_loss = self.perceptual_loss(real_imges, generated_images)\n",
        "    self.adv_loss = self.discrimenator_loss(output_labels, target_labels)\n",
        "    self.img_loss = self.image_loss(generated_images, real_imges)\n",
        "    \n",
        "    return self.img_loss + self.perc_loss + 0.001 * self.adv_loss\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1fTXrL8dXGM4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Discriminator Loss\n",
        "From the point of view of the discriminator loss It is considered a simple classification problem. The loss used is BCE."
      ]
    },
    {
      "metadata": {
        "id": "J6ib_SCeDRlR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Discriminator Loss { form-width: \"150px\" }\n",
        "from torch import nn\n",
        "\n",
        "class DiscriminatorLoss(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.loss_critrion = nn.BCEWithLogitsLoss()\n",
        "    \n",
        "  def forward(self, output_labels, target_labels):\n",
        "    return self.loss_critrion(output_labels, target_labels)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZdzbmQoxEq_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Save training and models state\n",
        "def save_state():\n",
        "  import datetime\n",
        "  import os\n",
        "\n",
        "  state = {\n",
        "      'epoch': epoch,\n",
        "      'discriminator_state_dict': D.state_dict(),\n",
        "      'generator_state_dict': G.state_dict(),\n",
        "      'training_results': training_results,\n",
        "      'DISCRIMINATOR_FINAL_FEATURE_MAP_SIZE': DISCRIMINATOR_FINAL_FEATURE_MAP_SIZE,\n",
        "      'RESIDUAL_BLOCKS': RESIDUAL_BLOCKS,\n",
        "      'UPSAMPLING_BLOCKS': UPSAMPLING_BLOCKS\n",
        "  }\n",
        "\n",
        "  file_name = 'model ' + str(datetime.datetime.now()) + '.pth'\n",
        "  file_path = os.path.join('/content/drive/My Drive/Models/', file_name)\n",
        "  torch.save(state, file_path)\n",
        "  return file_path\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_VVq9fL-zzW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Load training and models state\n",
        "def load_state(file_name):\n",
        "  import os\n",
        "  import torch\n",
        "  \n",
        "  saved_file_src = '/content/drive/My Drive/Models/'\n",
        "  file_path = os.path.join(saved_file_src, file_name)\n",
        "  if os.path.isfile(file_path):\n",
        "    return torch.load(file_path)\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEQ6soeICCQy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Models' Creation \n",
        "import torch.optim as optim\n",
        "\n",
        "D = Discriminator(DISCRIMINATOR_FINAL_FEATURE_MAP_SIZE)\n",
        "G = Generator(RESIDUAL_BLOCKS, UPSAMPLING_BLOCKS)\n",
        "\n",
        "\n",
        "D_loss = DiscriminatorLoss()\n",
        "G_loss = GeneratorLoss()\n",
        "\n",
        "\n",
        "# Create optimizers for the discriminator and generator\n",
        "d_optimizer = optim.SGD(D.parameters(), lr)\n",
        "g_optimizer = optim.Adam(G.parameters(), lr)\n",
        "\n",
        "###############################\n",
        "# Load training state if exists\n",
        "###############################\n",
        "file_name = 'mmodel 2019-03-16 16:42:36.967231.pth'\n",
        "state = load_state(file_name)\n",
        "\n",
        "old_state_exists = state is not None\n",
        "\n",
        "if old_state_exists:\n",
        "  print('loading old state from', file_name)\n",
        "  G.load_state_dict(state['generator_state_dict'])\n",
        "  D.load_state_dict(state['discriminator_state_dict'])\n",
        "else:\n",
        "  print(\"starting from the beginning\")\n",
        "\n",
        "  \n",
        "if train_on_gpu:\n",
        "  D, G = D.cuda(), G.cuda()\n",
        "  D_loss, G_loss = D_loss.cuda(), G_loss.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4g2Zq10gank",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T5VCoAyHXtm5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "The training process was powered by Google Colab with 40 epoches for training.\n",
        "\n",
        "##Optimizations \n",
        "\n",
        "\n",
        "*   Discriminator's training barch has either real or fake images not both\n",
        "*   The training process isn't symmetric, the generator or the discriminator may be trained more than the other depending on the accuracy of the discriminator\n",
        "*   Smoothing of the labels (real, fake) is applied\n",
        "*   Tanh activation function is applied to the generator\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p6-lmMhAXL0O",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title New Train Loop { form-width: \"150px\" }\n",
        "import random\n",
        "\n",
        "# Training\n",
        "INTERLEAV_TRAINING_LIMIT = -1\n",
        "# For logging the losses\n",
        "EPOCH_LOG_INTERVAL = 1\n",
        "BATCH_LOG_INTERVAL = 5\n",
        "SAVE_MODEL_INTERVAL = 2\n",
        "\n",
        "\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "G_LOSS = \"G_LOSS\"\n",
        "G_ADV_LOSS = \"G_ADV_LOSS\"\n",
        "G_PERC_LOSS = \"G_PERC_LOSS\"\n",
        "G_IMG_LOSS = \"G_IMG_LOSS\"\n",
        "G_TRAINING_ITERATIONS = \"G_TRAINING_ITERATIONS\"\n",
        "D_REAL_LOSS = \"D_REAL_LOSS\"\n",
        "D_FAKE_LOSS = \"D_FAKE_LOSS\"\n",
        "D_REAL_TRAINING_ITERATIONS = \"D_REAL_TRAINING_ITERATIONS\"\n",
        "D_FAKE_TRAINING_ITERATIONS = \"D_FAKE_TRAINING_ITERATIONS\"\n",
        "D_CORRECT_PREDICTIONS = \"D_CORRECT_PREDICTIONS\"\n",
        "CURRENT_TRAINED_IMAGES = \"CURRENT_TRAINED_IMAGES\"\n",
        "D_ACC = \"D_ACC\"\n",
        "\n",
        "\n",
        "if old_state_exists:\n",
        "  training_results = state['training_results']\n",
        "  START_EPOCH = state['epoch']\n",
        "else:\n",
        "  training_results = {\n",
        "      G_LOSS: [], G_ADV_LOSS: [], G_PERC_LOSS: [], G_IMG_LOSS: [], G_TRAINING_ITERATIONS: [],\n",
        "      D_REAL_LOSS: [], D_FAKE_LOSS: [], D_REAL_TRAINING_ITERATIONS: [], D_FAKE_TRAINING_ITERATIONS: [],\n",
        "      D_ACC: []\n",
        "  }\n",
        "  START_EPOCH = 1\n",
        "\n",
        "train_on_fake = True\n",
        "\n",
        "for epoch in range(START_EPOCH, EPOCH_NUM):\n",
        "   \n",
        "  running_results = {\n",
        "      G_LOSS: 0, G_ADV_LOSS: 0, G_PERC_LOSS: 0, G_IMG_LOSS: 0, G_TRAINING_ITERATIONS: 0,\n",
        "      D_REAL_LOSS: 0, D_FAKE_LOSS: 0, D_REAL_TRAINING_ITERATIONS: 0, D_FAKE_TRAINING_ITERATIONS: 0,\n",
        "      D_CORRECT_PREDICTIONS: 0, \n",
        "      CURRENT_TRAINED_IMAGES: 0\n",
        "  }\n",
        "                    \n",
        "  D.train()\n",
        "  G.train()\n",
        "  \n",
        "  for batch_id, (LR_images, HR_images) in enumerate(train_loader):\n",
        "    \n",
        "    if train_on_gpu:\n",
        "      HR_images, LR_images = HR_images.cuda(), LR_images.cuda()\n",
        "    \n",
        "    \n",
        "    ###############################\n",
        "    # Choose which netwrok to train\n",
        "    ###############################\n",
        "    \n",
        "    assert running_results[D_CORRECT_PREDICTIONS] <= running_results[CURRENT_TRAINED_IMAGES]\n",
        "    \n",
        "    try:\n",
        "      acc = running_results[D_CORRECT_PREDICTIONS] / running_results[CURRENT_TRAINED_IMAGES]\n",
        "    except:\n",
        "      acc = 0.5\n",
        "    \n",
        "    \n",
        "    g_train = acc > 0.3\n",
        "    d_train = acc < 0.85\n",
        "    \n",
        "      \n",
        "    \n",
        "    ###############################\n",
        "    # Train the Generator\n",
        "    ###############################\n",
        "    \n",
        "    if g_train:\n",
        "    \n",
        "      g_optimizer.zero_grad()\n",
        "\n",
        "      generated_image = G(LR_images)\n",
        "      D_fake_output = D(generated_image)\n",
        "\n",
        "      # The target is to make the discriminator belive that all the images are real\n",
        "      g_loss = G_loss(HR_images, generated_image, D_fake_output, torch.ones_like(D_fake_output) * 0.9)\n",
        "\n",
        "      g_loss.backward()\n",
        "      g_optimizer.step()\n",
        "      \n",
        "      running_results[G_LOSS] += g_loss.item() * BATCH_SIZE\n",
        "      running_results[G_ADV_LOSS] += G_loss.adv_loss.item() * BATCH_SIZE\n",
        "      running_results[G_PERC_LOSS] += G_loss.perc_loss.item() * BATCH_SIZE\n",
        "      running_results[G_IMG_LOSS] += G_loss.img_loss.item() * BATCH_SIZE\n",
        "      running_results[G_TRAINING_ITERATIONS] += 1\n",
        "      running_results[CURRENT_TRAINED_IMAGES] += BATCH_SIZE\n",
        "      running_results[D_CORRECT_PREDICTIONS] += (sigmoid(D_fake_output).cpu().detach().numpy()<=0.5).sum()\n",
        "      \n",
        "    ###############################\n",
        "    # Train the discriminator\n",
        "    ###############################\n",
        "    \n",
        "    if d_train:\n",
        "      \n",
        "      d_optimizer.zero_grad()\n",
        "      # If random number > 0.5 train on fake data else train on real\n",
        "      \n",
        "      if train_on_fake:\n",
        "        generated_image = G(LR_images)\n",
        "        D_fake_output = D(generated_image.detach())\n",
        "        # The goal is to make the discriminator get the fake images right with smooth factor\n",
        "        target = torch.zeros_like(D_fake_output) + 0.1\n",
        "        d_fake_loss = D_loss(D_fake_output, target)\n",
        "        d_fake_loss.backward()\n",
        "        \n",
        "        running_results[D_FAKE_LOSS] += d_fake_loss.item() * BATCH_SIZE\n",
        "        running_results[D_FAKE_TRAINING_ITERATIONS] += 1\n",
        "        running_results[D_CORRECT_PREDICTIONS] += (sigmoid(D_fake_output).cpu().detach().numpy()<=0.5).sum()\n",
        "      else:\n",
        "        D_real_output = D(HR_images)  \n",
        "        # The goal is to make the discriminator get the real images right with smooth factor\n",
        "        target = torch.ones_like(D_real_output) * 0.9\n",
        "        d_real_loss = D_loss(D_real_output, target)\n",
        "        d_real_loss.backward()\n",
        "        \n",
        "        running_results[D_REAL_LOSS] += d_real_loss.item() * BATCH_SIZE\n",
        "        running_results[D_REAL_TRAINING_ITERATIONS] += 1\n",
        "        running_results[D_CORRECT_PREDICTIONS] += (sigmoid(D_real_output).cpu().detach().numpy()>0.5).sum()\n",
        "\n",
        "      train_on_fake = not train_on_fake\n",
        "      d_optimizer.step()\n",
        "      running_results[CURRENT_TRAINED_IMAGES] += BATCH_SIZE\n",
        "    \n",
        "    ###############################\n",
        "    # Logging\n",
        "    ###############################\n",
        "      \n",
        "    total_d_iterations = running_results[D_REAL_TRAINING_ITERATIONS] + running_results[D_FAKE_TRAINING_ITERATIONS]\n",
        "    total_d_loss = running_results[D_REAL_LOSS] + running_results[D_FAKE_LOSS]\n",
        "    \n",
        "    g_images = running_results[G_TRAINING_ITERATIONS] * BATCH_SIZE + 1 \n",
        "    d_real_images = running_results[D_REAL_TRAINING_ITERATIONS] * BATCH_SIZE + 1\n",
        "    d_fake_images = (running_results[D_FAKE_TRAINING_ITERATIONS] * BATCH_SIZE + 1)\n",
        "    \n",
        "    if batch_id % BATCH_LOG_INTERVAL == 0:\n",
        "      print('[%d/%d/%d] Acc_D: %.4f Corr_D :%d Used_IMG_D: %d Loss_D: %.4f R_Loss_D: %.4f F_Loss_D: %.4f Loss_G: %.4f Adv_G: %.4f Perc_G: %.4f Img_G: %.4f D_Train: %d G_Train: %d' % (\n",
        "          batch_id,\n",
        "          epoch,\n",
        "          EPOCH_NUM,\n",
        "          \n",
        "          acc,\n",
        "          running_results[D_CORRECT_PREDICTIONS],\n",
        "          running_results[CURRENT_TRAINED_IMAGES],\n",
        "          \n",
        "          total_d_loss / (total_d_iterations * BATCH_SIZE),\n",
        "          running_results[D_REAL_LOSS] / d_real_images,\n",
        "          running_results[D_FAKE_LOSS] / d_fake_images,\n",
        "          \n",
        "          running_results[G_LOSS] / g_images,\n",
        "          running_results[G_ADV_LOSS] / g_images,\n",
        "          running_results[G_PERC_LOSS] / g_images,\n",
        "          running_results[G_IMG_LOSS] / g_images,\n",
        "          \n",
        "          total_d_iterations,\n",
        "          running_results[G_TRAINING_ITERATIONS]\n",
        "      ))\n",
        "\n",
        "  \n",
        "\n",
        "  if epoch % EPOCH_LOG_INTERVAL == 0:\n",
        "    \n",
        "    g_images = running_results[G_TRAINING_ITERATIONS] * BATCH_SIZE + 1 \n",
        "    d_real_images = running_results[D_REAL_TRAINING_ITERATIONS] * BATCH_SIZE + 1\n",
        "    d_fake_images = (running_results[D_FAKE_TRAINING_ITERATIONS] * BATCH_SIZE + 1)\n",
        "    \n",
        "    training_results[G_LOSS].append(running_results[G_LOSS] / g_images)\n",
        "    training_results[G_ADV_LOSS].append(running_results[G_ADV_LOSS] / g_images)\n",
        "    training_results[G_PERC_LOSS].append(running_results[G_PERC_LOSS] / g_images)\n",
        "    training_results[G_IMG_LOSS].append(running_results[G_IMG_LOSS] / g_images)\n",
        "    training_results[G_TRAINING_ITERATIONS].append(running_results[G_TRAINING_ITERATIONS])\n",
        "    training_results[D_REAL_LOSS].append(running_results[D_REAL_LOSS] / d_real_images)\n",
        "    training_results[D_FAKE_LOSS].append(running_results[D_FAKE_LOSS] / d_fake_images)\n",
        "    training_results[D_REAL_TRAINING_ITERATIONS].append(running_results[D_REAL_TRAINING_ITERATIONS])\n",
        "    training_results[D_FAKE_TRAINING_ITERATIONS].append(running_results[D_FAKE_TRAINING_ITERATIONS])\n",
        "    training_results[D_ACC].append(running_results[D_CORRECT_PREDICTIONS] / running_results[CURRENT_TRAINED_IMAGES] + 1)\n",
        "           \n",
        "  if epoch % SAVE_MODEL_INTERVAL == 0:\n",
        "    print(\"saving model state\", save_state())\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2Y2JRw8hpSo",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Validation and Results Reporting\n",
        "import os\n",
        "import math\n",
        "\n",
        "if not os.path.exists(\"/content/pytorch_ssim\"):\n",
        "  !git clone https://github.com/Po-Hsun-Su/pytorch-ssim.git\n",
        "  !mv /content/pytorch-ssim /content/pytorch_ssim\n",
        "\n",
        "from pytorch_ssim import pytorch_ssim\n",
        "  \n",
        "\n",
        "EPOCH_MSE = 'EPOCH_MSE'\n",
        "EPOCH_SSIM = 'EPOCH_SSIM'\n",
        "\n",
        "valing_results = {EPOCH_MSE: 0, EPOCH_SSIM: 0}\n",
        "dataset_size = len(validation_loader.dataset)\n",
        "\n",
        "for batch_id, (LR_images, HR_images) in enumerate(validation_loader):\n",
        "  \n",
        "  if train_on_gpu:\n",
        "    LR_images = LR_images.cuda()\n",
        "    HR_images = HR_images.cuda()\n",
        "  \n",
        "  SR = G(LR_images)\n",
        "\n",
        "  valing_results[EPOCH_MSE] += ((SR - HR_images) ** 2).data.mean() * BATCH_SIZE\n",
        "  valing_results[EPOCH_SSIM] += pytorch_ssim.ssim(SR, HR_images).data.mean() * BATCH_SIZE\n",
        "  \n",
        "total_mse_loss = valing_results[EPOCH_MSE] / dataset_size\n",
        "total_ssim_loss = valing_results[EPOCH_SSIM] / dataset_size\n",
        "psnr = 10 * math.log10(1 / total_mse_loss)\n",
        "\n",
        "print(\"MSE: %.4f SSIM: %.4f PSNR: %.4f\" %(\n",
        "    total_mse_loss,\n",
        "    total_ssim_loss,\n",
        "    psnr\n",
        "))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}